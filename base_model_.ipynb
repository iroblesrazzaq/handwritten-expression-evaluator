{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.features = self.data.iloc[:, :-1].values.astype(np.float32)\n",
    "        self.labels = self.data.iloc[:, -1].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.features[idx].reshape(64, 64)  # 64x64 images\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = torch.from_numpy(image).unsqueeze(0)  # Add channel dimension\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = ExpDataset('train_data.csv')\n",
    "val_data = ExpDataset('val_data.csv')\n",
    "test_data = ExpDataset('test_data.csv')\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 19  # 0-9, add, dec, div, eq, mul, sub, x, y, z\n",
    "model = CNN(num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Batch 100/101, Loss: 0.3185\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.7018, Train Accuracy: 0.7782\n",
      "Val Loss: 0.4042, Val Accuracy: 0.8727\n",
      "Learning Rate: 0.001000\n",
      "---\n",
      "Epoch 3/100, Batch 0/101, Loss: 0.3695\n",
      "Epoch 3/100, Batch 100/101, Loss: 0.4567\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.4543, Train Accuracy: 0.8561\n",
      "Val Loss: 0.3283, Val Accuracy: 0.9075\n",
      "Learning Rate: 0.001000\n",
      "---\n",
      "Epoch 4/100, Batch 0/101, Loss: 0.3796\n",
      "Epoch 4/100, Batch 100/101, Loss: 0.1928\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.3494, Train Accuracy: 0.8839\n",
      "Val Loss: 0.2219, Val Accuracy: 0.9385\n",
      "Learning Rate: 0.001000\n",
      "---\n",
      "Epoch 5/100, Batch 0/101, Loss: 0.3038\n",
      "Epoch 5/100, Batch 100/101, Loss: 0.1901\n",
      "Epoch 5/100:\n",
      "Train Loss: 0.2986, Train Accuracy: 0.9022\n",
      "Val Loss: 0.2520, Val Accuracy: 0.9373\n",
      "Learning Rate: 0.000100\n",
      "---\n",
      "Epoch 6/100, Batch 0/101, Loss: 0.1610\n",
      "Epoch 6/100, Batch 100/101, Loss: 0.1918\n",
      "Epoch 6/100:\n",
      "Train Loss: 0.1801, Train Accuracy: 0.9444\n",
      "Val Loss: 0.1557, Val Accuracy: 0.9634\n",
      "Learning Rate: 0.000100\n",
      "---\n",
      "Epoch 7/100, Batch 0/101, Loss: 0.1380\n",
      "Epoch 7/100, Batch 100/101, Loss: 0.0336\n",
      "Epoch 7/100:\n",
      "Train Loss: 0.1430, Train Accuracy: 0.9573\n",
      "Val Loss: 0.1448, Val Accuracy: 0.9640\n",
      "Learning Rate: 0.000100\n",
      "---\n",
      "Epoch 8/100, Batch 0/101, Loss: 0.1198\n",
      "Epoch 8/100, Batch 100/101, Loss: 0.1043\n",
      "Epoch 8/100:\n",
      "Train Loss: 0.1241, Train Accuracy: 0.9617\n",
      "Val Loss: 0.1297, Val Accuracy: 0.9721\n",
      "Learning Rate: 0.000100\n",
      "---\n",
      "Epoch 9/100, Batch 0/101, Loss: 0.1650\n",
      "Epoch 9/100, Batch 100/101, Loss: 0.1326\n",
      "Epoch 9/100:\n",
      "Train Loss: 0.1176, Train Accuracy: 0.9646\n",
      "Val Loss: 0.1260, Val Accuracy: 0.9696\n",
      "Learning Rate: 0.000100\n",
      "---\n",
      "Epoch 10/100, Batch 0/101, Loss: 0.0504\n",
      "Epoch 10/100, Batch 100/101, Loss: 0.0912\n",
      "Epoch 10/100:\n",
      "Train Loss: 0.1046, Train Accuracy: 0.9683\n",
      "Val Loss: 0.1222, Val Accuracy: 0.9721\n",
      "Learning Rate: 0.000010\n",
      "---\n",
      "Epoch 11/100, Batch 0/101, Loss: 0.1080\n",
      "Epoch 11/100, Batch 100/101, Loss: 0.0493\n",
      "Epoch 11/100:\n",
      "Train Loss: 0.1057, Train Accuracy: 0.9682\n",
      "Val Loss: 0.1224, Val Accuracy: 0.9714\n",
      "Learning Rate: 0.000010\n",
      "---\n",
      "Epoch 12/100, Batch 0/101, Loss: 0.1123\n",
      "Epoch 12/100, Batch 100/101, Loss: 0.1377\n",
      "Epoch 12/100:\n",
      "Train Loss: 0.1001, Train Accuracy: 0.9699\n",
      "Val Loss: 0.1219, Val Accuracy: 0.9721\n",
      "Learning Rate: 0.000010\n",
      "---\n",
      "Epoch 13/100, Batch 0/101, Loss: 0.1439\n",
      "Epoch 13/100, Batch 100/101, Loss: 0.0917\n",
      "Epoch 13/100:\n",
      "Train Loss: 0.0981, Train Accuracy: 0.9710\n",
      "Val Loss: 0.1210, Val Accuracy: 0.9727\n",
      "Learning Rate: 0.000010\n",
      "---\n",
      "Epoch 14/100, Batch 0/101, Loss: 0.0382\n",
      "Epoch 14/100, Batch 100/101, Loss: 0.0215\n",
      "Epoch 14/100:\n",
      "Train Loss: 0.0925, Train Accuracy: 0.9722\n",
      "Val Loss: 0.1202, Val Accuracy: 0.9727\n",
      "Learning Rate: 0.000010\n",
      "---\n",
      "Epoch 15/100, Batch 0/101, Loss: 0.0801\n",
      "Epoch 15/100, Batch 100/101, Loss: 0.1423\n",
      "Epoch 15/100:\n",
      "Train Loss: 0.0881, Train Accuracy: 0.9763\n",
      "Val Loss: 0.1193, Val Accuracy: 0.9745\n",
      "Learning Rate: 0.000001\n",
      "---\n",
      "Epoch 16/100, Batch 0/101, Loss: 0.0928\n",
      "Epoch 16/100, Batch 100/101, Loss: 0.0796\n",
      "Epoch 16/100:\n",
      "Train Loss: 0.0977, Train Accuracy: 0.9721\n",
      "Val Loss: 0.1202, Val Accuracy: 0.9733\n",
      "Learning Rate: 0.000001\n",
      "---\n",
      "Epoch 17/100, Batch 0/101, Loss: 0.0963\n",
      "Epoch 17/100, Batch 100/101, Loss: 0.1022\n",
      "Epoch 17/100:\n",
      "Train Loss: 0.0908, Train Accuracy: 0.9711\n",
      "Val Loss: 0.1205, Val Accuracy: 0.9739\n",
      "Learning Rate: 0.000001\n",
      "---\n",
      "Epoch 18/100, Batch 0/101, Loss: 0.1329\n",
      "Epoch 18/100, Batch 100/101, Loss: 0.0657\n",
      "Epoch 18/100:\n",
      "Train Loss: 0.0938, Train Accuracy: 0.9722\n",
      "Val Loss: 0.1186, Val Accuracy: 0.9733\n",
      "Learning Rate: 0.000001\n",
      "Early stopping triggered after 18 epochs\n"
     ]
    }
   ],
   "source": [
    " # Training setup\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "num_epochs = 100\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        train_total += target.size(0)\n",
    "        train_correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            _, predicted = output.max(1)\n",
    "            val_total += target.size(0)\n",
    "            val_correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "    \n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), 'normalized_model.pth')\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "    \n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0134, Test Accuracy: 0.9727\n",
      "\n",
      "Sample Predictions:\n",
      "Sample 1:\n",
      "Ground Truth: mul\n",
      "Prediction: mul\n",
      "Correct: True\n",
      "---\n",
      "Sample 2:\n",
      "Ground Truth: 4\n",
      "Prediction: 4\n",
      "Correct: True\n",
      "---\n",
      "Sample 3:\n",
      "Ground Truth: eq\n",
      "Prediction: eq\n",
      "Correct: True\n",
      "---\n",
      "Sample 4:\n",
      "Ground Truth: 7\n",
      "Prediction: 7\n",
      "Correct: True\n",
      "---\n",
      "Sample 5:\n",
      "Ground Truth: sub\n",
      "Prediction: sub\n",
      "Correct: True\n",
      "---\n",
      "Sample 6:\n",
      "Ground Truth: add\n",
      "Prediction: add\n",
      "Correct: True\n",
      "---\n",
      "Sample 7:\n",
      "Ground Truth: 9\n",
      "Prediction: 9\n",
      "Correct: True\n",
      "---\n",
      "Sample 8:\n",
      "Ground Truth: dec\n",
      "Prediction: dec\n",
      "Correct: True\n",
      "---\n",
      "Sample 9:\n",
      "Ground Truth: mul\n",
      "Prediction: mul\n",
      "Correct: True\n",
      "---\n",
      "Sample 10:\n",
      "Ground Truth: 3\n",
      "Prediction: 3\n",
      "Correct: True\n",
      "---\n",
      "Sample 11:\n",
      "Ground Truth: 1\n",
      "Prediction: 1\n",
      "Correct: True\n",
      "---\n",
      "Sample 12:\n",
      "Ground Truth: 0\n",
      "Prediction: 8\n",
      "Correct: False\n",
      "---\n",
      "Sample 13:\n",
      "Ground Truth: 7\n",
      "Prediction: 4\n",
      "Correct: False\n",
      "---\n",
      "Sample 14:\n",
      "Ground Truth: 2\n",
      "Prediction: 2\n",
      "Correct: True\n",
      "---\n",
      "Sample 15:\n",
      "Ground Truth: 2\n",
      "Prediction: 2\n",
      "Correct: True\n",
      "---\n",
      "Sample 16:\n",
      "Ground Truth: 6\n",
      "Prediction: 6\n",
      "Correct: True\n",
      "---\n",
      "Sample 17:\n",
      "Ground Truth: eq\n",
      "Prediction: eq\n",
      "Correct: True\n",
      "---\n",
      "Sample 18:\n",
      "Ground Truth: 6\n",
      "Prediction: 6\n",
      "Correct: True\n",
      "---\n",
      "Sample 19:\n",
      "Ground Truth: 4\n",
      "Prediction: 4\n",
      "Correct: True\n",
      "---\n",
      "Sample 20:\n",
      "Ground Truth: eq\n",
      "Prediction: eq\n",
      "Correct: True\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Load the best model for testing\n",
    "model.load_state_dict(torch.load('normalized_model.pth'))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "class_mapping = {\n",
    "    0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9',\n",
    "    10: 'add', 11: 'dec', 12: 'div', 13: 'eq', 14: 'mul', 15: 'sub', 16: 'x', 17: 'y', 18: 'z'\n",
    "}\n",
    "\n",
    "sample_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        _, predicted = output.max(1)\n",
    "        test_total += target.size(0)\n",
    "        test_correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        for true, pred in zip(target[:5], predicted[:5]):\n",
    "            sample_predictions.append((true.item(), pred.item()))\n",
    "        \n",
    "        if len(sample_predictions) >= 20:\n",
    "            break\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = test_correct / test_total\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i, (true, pred) in enumerate(sample_predictions[:20], 1):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"Ground Truth: {class_mapping[true]}\")\n",
    "    print(f\"Prediction: {class_mapping[pred]}\")\n",
    "    print(f\"Correct: {true == pred}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
